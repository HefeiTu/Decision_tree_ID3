{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    \"\"\"Calculates accuracy given two numpy arrays\"\"\"\n",
    "    \n",
    "    return np.mean(preds == labels) * 100\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    \"\"\"Creates a 10 x 10 confusion matrix given two numpy arrays\"\"\"\n",
    "    \n",
    "    matrix = np.zeros((10,10))\n",
    "    for i in range(len(preds)):\n",
    "        matrix[preds[i],labels[i]] += 1\n",
    "    return matrix    \n",
    "\n",
    "def precision_and_recall(preds, labels):\n",
    "    \"\"\"Returns individual Precision and Recall values of each class\"\"\"\n",
    "    \n",
    "    matrix = confusion_matrix(preds, labels)\n",
    "    r = []\n",
    "    p = []\n",
    "        \n",
    "    for i in range(10):\n",
    "        TP = float(matrix[i,i])\n",
    "        TP_FP = np.sum(matrix[i,:])\n",
    "        TP_FN = np.sum(matrix[:,i])\n",
    "        \n",
    "        recall = \"NA\" if TP_FN == 0 else TP / TP_FN\n",
    "        precision = \"NA\" if TP_FP == 0 else TP / TP_FP\n",
    "        r.append(recall)\n",
    "        p.append(precision)\n",
    "    \n",
    "    return (p, r)\n",
    "\n",
    "\n",
    "def fscore(preds, labels):\n",
    "    \"\"\"Calculates macro f score given two numpy arrays\"\"\"\n",
    "    \n",
    "    p, r = precision_and_recall(preds, labels)\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    count = 0.0\n",
    "    for x in p:\n",
    "        if x != \"NA\":\n",
    "            count += 1\n",
    "            precision += x\n",
    "    precision /= count\n",
    "    \n",
    "    count = 0.0\n",
    "    for x in r:\n",
    "        if x != \"NA\":\n",
    "            count += 1\n",
    "            recall += x\n",
    "    recall /= count\n",
    "    \n",
    "    return 2*precision*recall/(precision+recall)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_into_matrix(path):\n",
    "    df = []\n",
    "    colnames = None\n",
    "    with open(path, \"r\") as f:\n",
    "        colnames = {i:name[1:-1] for i, name in enumerate(f.readline().strip().split(';'))}\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            df.append([float(x) for x in line.strip().split(\";\")])\n",
    "            line = f.readline()\n",
    "    return colnames, np.asarray(df, dtype=float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Format of data (Numpy matrix)\n",
    "    - Each row is one sample\n",
    "    - Each column is a particular feature\n",
    "    - Last column is the label\n",
    "Header is the column names of all the features\n",
    "\"\"\"\n",
    "header, data = read_csv_into_matrix(\"winequality-white.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals(df, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return np.unique(df[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(df):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    return np.unique(df[:,-1], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        \"\"\"Stores column number and value of the question\"\"\"\n",
    "        \n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        \n",
    "    def match(self, row):\n",
    "        \"\"\"Answers question for a particular row\"\"\"\n",
    "        \n",
    "        if is_numeric(self.value):\n",
    "            return row[self.column] >= self.value\n",
    "        else:\n",
    "            return row[self.column] == self.value\n",
    "\n",
    "    def partition(self, df):\n",
    "        \"\"\"Splits given df based on question\"\"\"\n",
    "        \n",
    "        true_df = None\n",
    "        false_df = None\n",
    "        index = None\n",
    "        if is_numeric(self.value):\n",
    "            index = df[:,self.column] >= self.value\n",
    "        else:\n",
    "            index = df[:self.column] == self.value\n",
    "        true_df = df[index]\n",
    "        false_df = df[~index]\n",
    "        return true_df, false_df\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Helper method to print question in readable form\"\"\"\n",
    "        \n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is {} {} {} ?\".format(header[self.column], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on a toy dataset\n",
    "toy = np.random.randn(50, 10) * 100\n",
    "toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Question(1, 0)\n",
    "t, f = q.partition(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n",
      "(25, 10)\n",
      "(25, 10)\n"
     ]
    }
   ],
   "source": [
    "print(toy.shape)\n",
    "print(t.shape)\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(df):\n",
    "    \"\"\"Calculate entropy of data with respect to the last column (labels)\"\"\"\n",
    "    \n",
    "    labels, counts = class_counts(df)\n",
    "    probs = counts / float(df.shape[0])\n",
    "    entropy = - np.sum(probs * np.log2(probs))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8617149332355558"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left, right, current_entropy):\n",
    "    \"\"\"Information Gain.\n",
    "\n",
    "    The Entropy of the starting node, minus the weighted entropy of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(left.shape[0]) / (left.shape[0] + right.shape[0])\n",
    "    return current_entropy - p * entropy(left) - (1 - p) * entropy(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000009"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain(t, f, entropy(toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(df):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\n",
    "    Is surprisingly fast.\n",
    "    If slow:\n",
    "        - Try 1000 random values in a particular column instead of all values\n",
    "        - Dynamic Programming approach to find best split for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    best_gain = 0  \n",
    "    best_question = None \n",
    "    \n",
    "    current_entropy = entropy(df)\n",
    "    \n",
    "    # Loop over each feature and its value to find best info_gain\n",
    "    n_features = df.shape[1] - 1  \n",
    "    for col in range(n_features): \n",
    "        values = np.unique(df[:,col])  \n",
    "        for val in values:  \n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # Try splitting the dataset\n",
    "            true_rows, false_rows = question.partition(df)\n",
    "\n",
    "            # Skip this split if it doesn't divide the dataset.\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_entropy)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1433405667634391, Is alcohol >= 10.9 ?)\n"
     ]
    }
   ],
   "source": [
    "print(find_best_split(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "        \n",
    "    predictions is a dictionary of counts of classes in this leaf.\n",
    "    Output is usually the majority class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        counts = np.column_stack(class_counts(rows))\n",
    "        self.predictions = {row[0]:row[1] for row in counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Tree:\n",
    "    \n",
    "    def __init__(self, max_depth=32, min_sample_split=2):\n",
    "        self.depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        \n",
    "        \n",
    "    def train(self, df):\n",
    "        \"\"\"Builds tree based on dataset\"\"\"\n",
    "        self.root = self._build_tree(df, self.depth)\n",
    "        \n",
    "        \n",
    "    def _build_tree(self, df, depth):\n",
    "        \"\"\"Builds the tree recursively\"\"\"\n",
    "\n",
    "        # If number of samples is less than required, we cannot split.\n",
    "        if df.shape[0] < self.min_sample_split:\n",
    "            return Leaf(df)\n",
    "        \n",
    "        # Pick best question by maximizing information gain\n",
    "        gain, question = find_best_split(df)\n",
    "\n",
    "        # If there is not gain, or we have reached max depth\n",
    "        if gain == 0 or depth == 0:\n",
    "            return Leaf(df)\n",
    "        \n",
    "\n",
    "        # Partition and recursively build left and right branches\n",
    "        true_rows, false_rows = question.partition(df)\n",
    "        true_branch = self._build_tree(true_rows, depth-1)\n",
    "        false_branch = self._build_tree(false_rows, depth-1)\n",
    "\n",
    "        return Decision_Node(question, true_branch, false_branch)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        \"\"\"Returns prediction for single row\"\"\"\n",
    "        \n",
    "        prediction = self._classify(row, self.root)\n",
    "        return prediction\n",
    "    \n",
    "    def _classify(self, row, node):\n",
    "        \"\"\"Recursively goes down the tree to return prediction\"\"\"\n",
    "\n",
    "        # Base case: At a leaf node\n",
    "        if isinstance(node, Leaf):\n",
    "            maxcount = 0\n",
    "            maxlabel = None\n",
    "            for k, v in node.predictions.items():\n",
    "                if v >= maxcount:\n",
    "                    maxcount = v\n",
    "                    maxlabel = k\n",
    "            return maxlabel\n",
    "\n",
    "        # If not, match with question.\n",
    "        if node.question.match(row):\n",
    "            return self._classify(row, node.true_branch)\n",
    "        else:\n",
    "            return self._classify(row, node.false_branch) \n",
    "        \n",
    "    def test(self, df, labels):\n",
    "        \"\"\"Returns accuracy and fscore of test dataset\"\"\"\n",
    "        # Not vectorized.\n",
    "        \n",
    "        preds = np.zeros(df.shape[0])\n",
    "        for i, row in enumerate(df):\n",
    "            preds[i] = self.predict(row)\n",
    "            \n",
    "        a = accuracy(preds.astype(int), labels.astype(int))\n",
    "        f = fscore(preds.astype(int), labels.astype(int))\n",
    "        \n",
    "        return a, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: Max depth, Min number of splits.\n",
    "model = Decision_Tree(32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train takes data and labels together\n",
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95.48795426704777, 0.5876952659660883)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test takes data and labels separately.\n",
    "model.test(data[:,:-1], data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters:\n",
      "Depth: 18\n",
      "Min-samples split: 3\n",
      "\n",
      "Fold-1:\n",
      "Train: F1 Score: 91.0, Accuracy: 97.1\n",
      "Validation: F1 Score: 33.0, Accuracy: 55.1\n",
      "Test: F1 Score: 33.3, Accuracy: 56.7\n",
      "\n",
      "Fold-2:\n",
      "Train: F1 Score: 93.9, Accuracy: 98.0\n",
      "Validation: F1 Score: 32.7, Accuracy: 53.7\n",
      "Test: F1 Score: 39.2, Accuracy: 57.1\n",
      "\n",
      "Fold-3:\n",
      "Train: F1 Score: 92.9, Accuracy: 97.6\n",
      "Validation: F1 Score: 31.7, Accuracy: 53.5\n",
      "Test: F1 Score: 36.8, Accuracy: 58.9\n",
      "\n",
      "Fold-4:\n",
      "Train: F1 Score: 90.1, Accuracy: 97.3\n",
      "Validation: F1 Score: 36.7, Accuracy: 55.9\n",
      "Test: F1 Score: 36.7, Accuracy: 57.2\n",
      "\n",
      "Average:\n",
      "Train: F1 Score: 92.0, Accuracy: 97.5\n",
      "Validation: F1 Score: 33.5, Accuracy: 54.6\n",
      "Test: F1 Score: 36.5, Accuracy: 57.5\n",
      "\n",
      "Hyper-parameters:\n",
      "Depth: 20\n",
      "Min-samples split: 3\n",
      "\n",
      "Fold-1:\n",
      "Train: F1 Score: 91.8, Accuracy: 97.8\n",
      "Validation: F1 Score: 32.7, Accuracy: 54.7\n",
      "Test: F1 Score: 33.0, Accuracy: 56.5\n",
      "\n",
      "Fold-2:\n",
      "Train: F1 Score: 94.1, Accuracy: 98.5\n",
      "Validation: F1 Score: 32.6, Accuracy: 53.6\n",
      "Test: F1 Score: 39.2, Accuracy: 57.1\n",
      "\n",
      "Fold-3:\n",
      "Train: F1 Score: 93.2, Accuracy: 98.3\n",
      "Validation: F1 Score: 31.9, Accuracy: 53.9\n",
      "Test: F1 Score: 36.9, Accuracy: 59.1\n",
      "\n",
      "Fold-4:\n",
      "Train: F1 Score: 90.5, Accuracy: 98.1\n",
      "Validation: F1 Score: 36.8, Accuracy: 56.1\n",
      "Test: F1 Score: 36.9, Accuracy: 57.6\n",
      "\n",
      "Average:\n",
      "Train: F1 Score: 92.4, Accuracy: 98.2\n",
      "Validation: F1 Score: 33.5, Accuracy: 54.6\n",
      "Test: F1 Score: 36.5, Accuracy: 57.6\n",
      "\n",
      "Hyper-parameters:\n",
      "Depth: 22\n",
      "Min-samples split: 3\n",
      "\n",
      "Fold-1:\n",
      "Train: F1 Score: 92.1, Accuracy: 98.2\n",
      "Validation: F1 Score: 32.6, Accuracy: 54.6\n",
      "Test: F1 Score: 33.0, Accuracy: 56.6\n",
      "\n",
      "Fold-2:\n",
      "Train: F1 Score: 94.2, Accuracy: 98.7\n",
      "Validation: F1 Score: 32.7, Accuracy: 53.9\n",
      "Test: F1 Score: 39.1, Accuracy: 56.9\n",
      "\n",
      "Fold-3:\n",
      "Train: F1 Score: 93.4, Accuracy: 98.6\n",
      "Validation: F1 Score: 31.9, Accuracy: 53.9\n",
      "Test: F1 Score: 36.9, Accuracy: 59.1\n",
      "\n",
      "Fold-4:\n",
      "Train: F1 Score: 90.6, Accuracy: 98.4\n",
      "Validation: F1 Score: 36.8, Accuracy: 56.2\n",
      "Test: F1 Score: 37.0, Accuracy: 57.8\n",
      "\n",
      "Average:\n",
      "Train: F1 Score: 92.5, Accuracy: 98.5\n",
      "Validation: F1 Score: 33.5, Accuracy: 54.6\n",
      "Test: F1 Score: 36.5, Accuracy: 57.6\n",
      "\n",
      "Hyper-parameters:\n",
      "Depth: 24\n",
      "Min-samples split: 3\n",
      "\n",
      "Fold-1:\n",
      "Train: F1 Score: 92.1, Accuracy: 98.3\n",
      "Validation: F1 Score: 32.6, Accuracy: 54.7\n",
      "Test: F1 Score: 32.9, Accuracy: 56.5\n",
      "\n",
      "Fold-2:\n",
      "Train: F1 Score: 94.2, Accuracy: 98.7\n",
      "Validation: F1 Score: 32.7, Accuracy: 53.9\n",
      "Test: F1 Score: 39.1, Accuracy: 56.9\n",
      "\n",
      "Fold-3:\n",
      "Train: F1 Score: 93.4, Accuracy: 98.7\n",
      "Validation: F1 Score: 31.9, Accuracy: 54.0\n",
      "Test: F1 Score: 36.9, Accuracy: 59.2\n",
      "\n",
      "Fold-4:\n",
      "Train: F1 Score: 90.6, Accuracy: 98.5\n",
      "Validation: F1 Score: 36.8, Accuracy: 56.2\n",
      "Test: F1 Score: 37.0, Accuracy: 57.9\n",
      "\n",
      "Average:\n",
      "Train: F1 Score: 92.6, Accuracy: 98.5\n",
      "Validation: F1 Score: 33.5, Accuracy: 54.7\n",
      "Test: F1 Score: 36.5, Accuracy: 57.6\n",
      "\n",
      "Hyper-parameters:\n",
      "Depth: 26\n",
      "Min-samples split: 3\n",
      "\n",
      "Fold-1:\n",
      "Train: F1 Score: 92.1, Accuracy: 98.3\n",
      "Validation: F1 Score: 32.6, Accuracy: 54.6\n",
      "Test: F1 Score: 33.0, Accuracy: 56.7\n",
      "\n",
      "Fold-2:\n",
      "Train: F1 Score: 94.2, Accuracy: 98.7\n",
      "Validation: F1 Score: 32.7, Accuracy: 53.9\n",
      "Test: F1 Score: 39.1, Accuracy: 56.9\n",
      "\n",
      "Fold-3:\n",
      "Train: F1 Score: 93.4, Accuracy: 98.8\n",
      "Validation: F1 Score: 31.9, Accuracy: 54.0\n",
      "Test: F1 Score: 36.9, Accuracy: 59.2\n",
      "\n",
      "Fold-4:\n",
      "Train: F1 Score: 90.6, Accuracy: 98.6\n",
      "Validation: F1 Score: 36.8, Accuracy: 56.2\n",
      "Test: F1 Score: 37.0, Accuracy: 57.8\n",
      "\n",
      "Average:\n",
      "Train: F1 Score: 92.6, Accuracy: 98.6\n",
      "Validation: F1 Score: 33.5, Accuracy: 54.7\n",
      "Test: F1 Score: 36.5, Accuracy: 57.6\n"
     ]
    }
   ],
   "source": [
    "def create_folds(data, num_folds):\n",
    "    \"\"\"Splits the given data into num_folds folds\"\"\"\n",
    "    # Initial shuffle\n",
    "    np.random.shuffle(data)\n",
    "    # Number of points in each fold\n",
    "    cut = int(math.ceil(len(data)/float(num_folds)))\n",
    "\n",
    "    # Divide the original data into folds and save each separately.\n",
    "    folds = []\n",
    "    for i in range(0, len(data), cut):\n",
    "        folds.append(data[i:i+cut,:])\n",
    "    return folds\n",
    "\n",
    "\n",
    "# 4 Fold cross validation\n",
    "num_folds = 4\n",
    "folds = create_folds(data, num_folds)\n",
    "\n",
    "# Ratio of train_data to use as validation\n",
    "ratio = 0.2\n",
    "\n",
    "    \n",
    "result_f = []\n",
    "result_a = []\n",
    "for minsample in [3]:\n",
    "    for depth in range(18,27,2):  \n",
    "        print \"\"\n",
    "        print \"Hyper-parameters:\"\n",
    "        print \"Depth: {}\".format(depth)\n",
    "        print \"Min-samples split: {}\".format(minsample)\n",
    "\n",
    "        train_a_list = []\n",
    "        train_f_list = []\n",
    "        valid_f_list = []\n",
    "        valid_a_list = []\n",
    "        test_a_list = []\n",
    "        test_f_list = []\n",
    "\n",
    "        #Loop over all folds using different test sets each time\n",
    "        for i in range(num_folds):\n",
    "\n",
    "            test_data = folds[i]\n",
    "            # Anything other than the test fold is used as training\n",
    "            train_folds = [x for x in range(num_folds) if x != i]\n",
    "            rem_data = folds[train_folds[0]]\n",
    "            for x in train_folds[1:]:\n",
    "                rem_data = np.concatenate((rem_data, folds[x]))\n",
    "\n",
    "            # Split train and validation depending on ratio.\n",
    "            split = int(len(rem_data)*(1-ratio))\n",
    "            train_data = rem_data[:split,:]\n",
    "            validation_data = rem_data[split:,:]\n",
    "\n",
    "            # Training\n",
    "            model = Decision_Tree(depth, minsample)\n",
    "            model.train(train_data)\n",
    "\n",
    "            # Validation and testing\n",
    "            train_acc, train_f = model.test(train_data[:,:-1], train_data[:,-1])\n",
    "            valid_acc, valid_f = model.test(validation_data[:,:-1], validation_data[:,-1])        \n",
    "            test_acc, test_f = model.test(test_data[:,:-1], test_data[:,-1])\n",
    "\n",
    "\n",
    "            print \"\\nFold-{}:\".format(i+1)\n",
    "            print \"Train: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(train_f, train_acc)\n",
    "            print \"Validation: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(valid_f, valid_acc)\n",
    "            print \"Test: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(test_f, test_acc)\n",
    "\n",
    "            train_a_list.append(train_acc)\n",
    "            train_f_list.append(train_f)\n",
    "            valid_f_list.append(valid_f)\n",
    "            valid_a_list.append(valid_acc)\n",
    "            test_a_list.append(test_acc)\n",
    "            test_f_list.append(test_f)\n",
    "\n",
    "\n",
    "        # Averaging\n",
    "        avg_train_acc = sum(train_a_list) / 4\n",
    "        avg_train_f = sum(train_f_list) / 4\n",
    "        avg_v_acc = sum(valid_a_list) / 4\n",
    "        avg_v_f = sum(valid_f_list) / 4\n",
    "        avg_t_acc = sum(test_a_list) / 4\n",
    "        avg_t_f = sum(test_f_list) / 4\n",
    "\n",
    "        print \"\\nAverage:\"\n",
    "        print \"Train: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(avg_train_f, avg_train_acc)\n",
    "        print \"Validation: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(avg_v_f, avg_v_acc)\n",
    "        print \"Test: F1 Score: {:.1f}, Accuracy: {:.1f}\".format(avg_t_f, avg_t_acc)\n",
    "\n",
    "        result_f.append(avg_v_f)\n",
    "        result_a.append(avg_v_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
